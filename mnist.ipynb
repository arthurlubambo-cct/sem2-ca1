{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd44d7a3-349b-4793-b42a-ae3d0b32e50c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 23:15:43.996917: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-05 23:15:44.939774: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-05 23:15:44.939814: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-05 23:15:44.946396: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-05 23:15:45.583747: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-05 23:15:45.587951: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-05 23:15:48.308791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a5e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from pyspark.sql.types import StructType,StructField, StringType, FloatType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9bea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3df554-27da-41e1-9756-0cedc4998fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train, test load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize values to [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c42cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PY SPARK VERSION OF THE DATAFRAME\n",
    "# def get_ps_dataFrame(x_data,y_data):\n",
    "#     data_unified = [( x_data[i].tolist(), str(y_data[i])) for i in range(len(x_data))]\n",
    "#     schema = StructType([ \n",
    "#         StructField(\"image\",ArrayType(ArrayType(FloatType())),False), \n",
    "#         StructField(\"class\", StringType(), False) \n",
    "#       ])\n",
    "#     df = spark.createDataFrame(data=data_unified,schema=schema)\n",
    "#     return df\n",
    "# train_df = get_ps_dataFrame(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0ede4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa5888ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pd_dataFrame(x_data,y_data):\n",
    "    return pd.DataFrame(data={'image':x_data.tolist(), 'class':y_data.tolist()})\n",
    "train_df = get_pd_dataFrame(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38eeb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_cross_same_class_pair(df:pd.DataFrame):\n",
    "    sorted_df = df.sort_values(\"class\").reset_index(drop=True)\n",
    "    shuffled_sorted_df = sorted_df.sample(frac=1).reset_index(drop=True).sort_values(\"class\").reset_index(drop=True)\n",
    "    shuffled_sorted_df.rename(columns={'image':'imagePair',\"class\": \"classPair\"}, inplace=True)\n",
    "    result = pd.concat([sorted_df, shuffled_sorted_df], axis=1)\n",
    "    return result[['image','imagePair', 'class']].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "same_class_cross_df = augment_data_cross_same_class_pair(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1875063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def series_to_tensor(series):\n",
    "    tensor_list = series.apply(lambda x: tf.constant(x))\n",
    "    return tf.stack(tensor_list)\n",
    "    \n",
    "def prepare_train_traditional_autoencoder_data(train_df):\n",
    "    image_tensor = series_to_tensor(train_df['image'])\n",
    "    in_x_train = image_tensor\n",
    "    out_x_train = image_tensor\n",
    "    return [in_x_train, out_x_train]\n",
    "\n",
    "def prepare_train_cross_same_class_autoencoder_data(train_df, additional_same_class_df):\n",
    "    original_df =  train_df.copy()\n",
    "    original_df[\"imagePair\"] = original_df[\"image\"]\n",
    "    concat_df = pd.concat([original_df, additional_same_class_df])\n",
    "    concat_df = concat_df.sample(frac=1).reset_index(drop=True)\n",
    "    in_x_train = series_to_tensor(concat_df['image'])\n",
    "    out_x_train = series_to_tensor(concat_df['imagePair'])\n",
    "    return [in_x_train, out_x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9f2999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[trad_ae_x_in,  trad_ae_x_out]= prepare_train_traditional_autoencoder_data(train_df)\n",
    "\n",
    "[cross_same_class_ae_x_in,  cross_same_class_ae_x_out]= prepare_train_cross_same_class_autoencoder_data(train_df, same_class_cross_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fce0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "class Autoencoder(keras.Model):\n",
    "    def __init__(self, latent_dim, input_shape ):\n",
    "        super(Autoencoder, self).__init__(input_shape,input_shape)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.shape = input_shape\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            input_shape,\n",
    "            layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2,2), padding='same'),\n",
    "            layers.Conv2D(8, (3, 3), activation='relu', padding='same'),\n",
    "            layers.MaxPooling2D((2,2), padding='same'),\n",
    "            layers.Conv2D(8, (3, 3), activation='relu', padding='same'),\n",
    "            layers.MaxPooling2D((2,2), padding='same'),\n",
    "        ])\n",
    "        encoder_output_shape = self.encoder.layers[-1].output_shape\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Conv2D(8, (3, 3), activation='relu', padding='same', input_shape=encoder_output_shape[1:]),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2D(8, (3, 3), activation='relu', padding='same'),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs,training = False):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "input_shape = keras.Input(shape=(28, 28, 1))\n",
    "latent_dim = 2\n",
    "ae = Autoencoder(latent_dim, input_shape)\n",
    "ae.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aee4ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 4, 4, 8)           1904      \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 28, 28, 1)         2481      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4385 (17.13 KB)\n",
      "Trainable params: 4385 (17.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7db29194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "600/600 [==============================] - 19s 31ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 18s 29ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 17s 29ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 17s 29ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 17s 29ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.1120 - val_loss: 0.1140\n"
     ]
    }
   ],
   "source": [
    "# train autoencoder\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# specify the path where you want to save your models\n",
    "checkpoint_file_path =  './checkpoints/ae'\n",
    "\n",
    "# create a model checkpoint callback\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_file_path, \n",
    "    monitor='val_loss',\n",
    "    verbose=0, \n",
    "    save_weights_only=True, \n",
    "    save_best_only=False,\n",
    "    mode='min', \n",
    "    save_freq=\"epoch\")\n",
    "\n",
    "history = ae.fit(trad_ae_x_in, trad_ae_x_in,\n",
    "                epochs=10,#100\n",
    "                batch_size=100,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[checkpoint] ,\n",
    "                workers=8,\n",
    "                use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "648d7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LOAD SAVED MODEL FROM CHECK POINT\n",
    "# ae.load_weights(checkpoint_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35aa4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19b05111-3762-494b-8629-6217d32f71cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss ae\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "258dcca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rand_score': 0.8733427542754275, 'f1_score': 0.565182956738704}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.cluster import rand_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_embed_vectors(model,x_data):\n",
    "    #run encoder to get embeded vector in latent space     \n",
    "    result = model.encoder.predict(x_data)\n",
    "    #flat array into 1d array (vector)     \n",
    "    return np.array(list(map(lambda x: x.flatten(), result)))\n",
    "\n",
    "def calculate_centroids(vectors, y_data):\n",
    "    classes = np.unique(y_data)\n",
    "    class_vectors = {}\n",
    "    for img_class in classes:\n",
    "        class_vectors[img_class] = np.mean(vectors[np.where(y_data == img_class)], axis=0)        \n",
    "    return class_vectors\n",
    "\n",
    "def get_estimated_class(model, x_data, y_data):\n",
    "    vectors = calculate_embed_vectors(model,x_data)\n",
    "    class_centroids = calculate_centroids(vectors, y_data)\n",
    "    classes = np.array(list(class_centroids.keys()))\n",
    "    centroids = np.array(list(class_centroids.values()))\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(centroids,classes)\n",
    "    return knn.predict(vectors)\n",
    "\n",
    "def calculate_rand_index(model, x_data, y_data):\n",
    "    labels_pred = get_estimated_class(model, x_data, y_data)\n",
    "    return {\n",
    "        'rand_score':rand_score(y_data,labels_pred),\n",
    "        'f1_score':f1_score(y_data,labels_pred,average='macro')\n",
    "           } \n",
    "    \n",
    "calculate_rand_index(ae, x_test, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "583c516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scae = Autoencoder(latent_dim, input_shape)\n",
    "scae.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78e99f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1200/1200 [==============================] - 34s 28ms/step - loss: 0.1131 - val_loss: 0.1140\n",
      "Epoch 2/10\n",
      "1200/1200 [==============================] - 33s 28ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 3/10\n",
      "1200/1200 [==============================] - 33s 28ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 4/10\n",
      "1200/1200 [==============================] - 33s 28ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 5/10\n",
      "1200/1200 [==============================] - 33s 28ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 6/10\n",
      "1200/1200 [==============================] - 33s 28ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 7/10\n",
      "1200/1200 [==============================] - 33s 27ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 8/10\n",
      "1200/1200 [==============================] - 34s 28ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 9/10\n",
      "1200/1200 [==============================] - 33s 28ms/step - loss: 0.1120 - val_loss: 0.1140\n",
      "Epoch 10/10\n",
      "1200/1200 [==============================] - 33s 28ms/step - loss: 0.1120 - val_loss: 0.1140\n"
     ]
    }
   ],
   "source": [
    "# train autoencoder\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# specify the path where you want to save your models\n",
    "checkpoint_sc_file_path =  './checkpoints/scae'\n",
    "\n",
    "# create a model checkpoint callback\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_sc_file_path, \n",
    "    monitor='val_loss',\n",
    "    verbose=0, \n",
    "    save_weights_only=True, \n",
    "    save_best_only=False,\n",
    "    mode='min', \n",
    "    save_freq=\"epoch\")\n",
    "\n",
    "history = scae.fit(cross_same_class_ae_x_in, cross_same_class_ae_x_out,\n",
    "                epochs=10,#100\n",
    "                batch_size=100,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[checkpoint] ,\n",
    "                workers=8,\n",
    "                use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "165ba94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rand_score': 0.8766336833683368, 'f1_score': 0.6074735790267998}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_rand_index(scae, x_test, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62ae07bc-3346-4852-9eaf-923a077549f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare data aumentated for same class match\n",
    "# train SCAE \n",
    "# plot loss SCAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c353b218-6261-46ec-84ca-beb69852f5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate encoded_vector_ae using autoencoder\n",
    "# calculate encoded_vector_vae using Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aea79c74-becf-4c31-bc64-02559ca94498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create function to calculate metric topK class metric\n",
    "# calculateMetric(encoded_vector_ae)\n",
    "# calculateMetric(encoded_vector_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24676f1e-eae5-4ac4-928e-2527739e3233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data visualisation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b28ecc1-7480-4b90-b15c-703302195d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# discuss results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a3847-9a99-4b6e-9939-8ee23bd781cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is time, apply in another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e405144b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
